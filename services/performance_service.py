"""
è»Šè¡Œå¯¶ CRM v5.1 - æ€§èƒ½å„ªåŒ–æœå‹™
åŒ—æ–—ä¸ƒæ˜Ÿæ–‡å‰µæ•¸ä½ Ã— ç¹”æ˜

åŠŸèƒ½ï¼š
1. è³‡æ–™åº«é€£æ¥æ± 
2. æŸ¥è©¢å„ªåŒ–èˆ‡åˆ†æ
3. ç´¢å¼•å»ºè­°
4. æ‰¹é‡æ“ä½œå„ªåŒ–
5. æ…¢æŸ¥è©¢æª¢æ¸¬
"""
import sqlite3
import time
import threading
import queue
import logging
from functools import wraps
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from contextlib import contextmanager

logger = logging.getLogger(__name__)


# ============================================================
# 1. è³‡æ–™åº«é€£æ¥æ± 
# ============================================================

class ConnectionPool:
    """SQLite é€£æ¥æ± 
    
    é›–ç„¶ SQLite æ˜¯å–®æª”æ¡ˆè³‡æ–™åº«ï¼Œé€£æ¥æ± ä»èƒ½ï¼š
    - æ¸›å°‘é€£æ¥å»ºç«‹é–‹éŠ·
    - æ§åˆ¶ä¸¦ç™¼é€£æ¥æ•¸
    - çµ±ä¸€é€£æ¥é…ç½®
    """
    
    def __init__(self, db_path: str, max_connections: int = 10, timeout: float = 30.0):
        """
        Args:
            db_path: è³‡æ–™åº«è·¯å¾‘
            max_connections: æœ€å¤§é€£æ¥æ•¸
            timeout: å–å¾—é€£æ¥çš„è¶…æ™‚æ™‚é–“
        """
        self.db_path = db_path
        self.max_connections = max_connections
        self.timeout = timeout
        self._pool = queue.Queue(maxsize=max_connections)
        self._lock = threading.Lock()
        self._created = 0
        self._in_use = 0
        
        # çµ±è¨ˆ
        self.stats = {
            'total_requests': 0,
            'pool_hits': 0,
            'pool_misses': 0,
            'timeouts': 0
        }
    
    def _create_connection(self) -> sqlite3.Connection:
        """å‰µå»ºæ–°é€£æ¥"""
        conn = sqlite3.connect(
            self.db_path,
            check_same_thread=False,
            timeout=self.timeout
        )
        conn.row_factory = sqlite3.Row
        
        # å„ªåŒ–é…ç½®
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        conn.execute("PRAGMA cache_size=-64000")  # 64MB
        conn.execute("PRAGMA temp_store=MEMORY")
        conn.execute("PRAGMA mmap_size=268435456")  # 256MB
        
        return conn
    
    def get_connection(self) -> sqlite3.Connection:
        """å–å¾—é€£æ¥"""
        self.stats['total_requests'] += 1
        
        try:
            # å˜—è©¦å¾æ± ä¸­å–å¾—
            conn = self._pool.get_nowait()
            self.stats['pool_hits'] += 1
            self._in_use += 1
            return conn
        except queue.Empty:
            pass
        
        # æ± ä¸­ç„¡å¯ç”¨é€£æ¥
        with self._lock:
            if self._created < self.max_connections:
                # å‰µå»ºæ–°é€£æ¥
                conn = self._create_connection()
                self._created += 1
                self._in_use += 1
                self.stats['pool_misses'] += 1
                return conn
        
        # é”åˆ°æœ€å¤§é€£æ¥æ•¸ï¼Œç­‰å¾…
        try:
            conn = self._pool.get(timeout=self.timeout)
            self.stats['pool_hits'] += 1
            self._in_use += 1
            return conn
        except queue.Empty:
            self.stats['timeouts'] += 1
            raise TimeoutError("ç„¡æ³•å–å¾—è³‡æ–™åº«é€£æ¥")
    
    def return_connection(self, conn: sqlite3.Connection):
        """æ­¸é‚„é€£æ¥"""
        self._in_use -= 1
        try:
            self._pool.put_nowait(conn)
        except queue.Full:
            # æ± å·²æ»¿ï¼Œé—œé–‰é€£æ¥
            conn.close()
            with self._lock:
                self._created -= 1
    
    @contextmanager
    def connection(self):
        """é€£æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = self.get_connection()
        try:
            yield conn
        finally:
            self.return_connection(conn)
    
    def get_stats(self) -> Dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        return {
            **self.stats,
            'pool_size': self._pool.qsize(),
            'in_use': self._in_use,
            'created': self._created,
            'hit_rate': round(self.stats['pool_hits'] / max(self.stats['total_requests'], 1) * 100, 2)
        }
    
    def close_all(self):
        """é—œé–‰æ‰€æœ‰é€£æ¥"""
        while not self._pool.empty():
            try:
                conn = self._pool.get_nowait()
                conn.close()
            except queue.Empty:
                break
        self._created = 0


# å…¨åŸŸé€£æ¥æ± 
_pools: Dict[str, ConnectionPool] = {}
_pools_lock = threading.Lock()


def get_pool(db_path: str, max_connections: int = 10) -> ConnectionPool:
    """å–å¾—æˆ–å‰µå»ºé€£æ¥æ± """
    with _pools_lock:
        if db_path not in _pools:
            _pools[db_path] = ConnectionPool(db_path, max_connections)
        return _pools[db_path]


# ============================================================
# 2. æŸ¥è©¢æ•ˆèƒ½åˆ†æ
# ============================================================

@dataclass
class QueryStats:
    """æŸ¥è©¢çµ±è¨ˆ"""
    sql: str
    execution_time: float
    rows_affected: int
    timestamp: datetime = field(default_factory=datetime.now)
    explain_plan: str = ""


class QueryAnalyzer:
    """æŸ¥è©¢åˆ†æå™¨"""
    
    def __init__(self, slow_threshold: float = 0.5):
        """
        Args:
            slow_threshold: æ…¢æŸ¥è©¢é–¾å€¼ï¼ˆç§’ï¼‰
        """
        self.slow_threshold = slow_threshold
        self.query_history: List[QueryStats] = []
        self.slow_queries: List[QueryStats] = []
        self._lock = threading.Lock()
        self.max_history = 1000
    
    def record_query(self, sql: str, execution_time: float, rows_affected: int = 0):
        """è¨˜éŒ„æŸ¥è©¢"""
        stats = QueryStats(
            sql=sql[:500],  # æˆªæ–·éé•· SQL
            execution_time=execution_time,
            rows_affected=rows_affected
        )
        
        with self._lock:
            self.query_history.append(stats)
            if len(self.query_history) > self.max_history:
                self.query_history.pop(0)
            
            if execution_time >= self.slow_threshold:
                self.slow_queries.append(stats)
                if len(self.slow_queries) > 100:
                    self.slow_queries.pop(0)
                logger.warning(f"æ…¢æŸ¥è©¢ ({execution_time:.3f}s): {sql[:100]}...")
    
    def get_slow_queries(self, limit: int = 20) -> List[Dict]:
        """å–å¾—æ…¢æŸ¥è©¢åˆ—è¡¨"""
        with self._lock:
            return [
                {
                    'sql': q.sql,
                    'time': round(q.execution_time, 3),
                    'rows': q.rows_affected,
                    'timestamp': q.timestamp.isoformat()
                }
                for q in sorted(self.slow_queries, key=lambda x: x.execution_time, reverse=True)[:limit]
            ]
    
    def get_statistics(self) -> Dict:
        """å–å¾—æŸ¥è©¢çµ±è¨ˆ"""
        with self._lock:
            if not self.query_history:
                return {'total': 0}
            
            times = [q.execution_time for q in self.query_history]
            return {
                'total': len(self.query_history),
                'slow_count': len(self.slow_queries),
                'avg_time': round(sum(times) / len(times), 4),
                'max_time': round(max(times), 4),
                'min_time': round(min(times), 4),
                'p95_time': round(sorted(times)[int(len(times) * 0.95)] if times else 0, 4)
            }


# å…¨åŸŸæŸ¥è©¢åˆ†æå™¨
_query_analyzer = QueryAnalyzer()


def timed_query(func: Callable) -> Callable:
    """æŸ¥è©¢è¨ˆæ™‚è£é£¾å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            elapsed = time.perf_counter() - start
            # å˜—è©¦æå– SQL
            sql = args[1] if len(args) > 1 else kwargs.get('sql', str(func.__name__))
            _query_analyzer.record_query(str(sql), elapsed)
    return wrapper


# ============================================================
# 3. ç´¢å¼•åˆ†æèˆ‡å»ºè­°
# ============================================================

class IndexAdvisor:
    """ç´¢å¼•å»ºè­°å™¨"""
    
    def __init__(self, conn: sqlite3.Connection):
        self.conn = conn
    
    def analyze_table(self, table_name: str) -> Dict:
        """åˆ†æè¡¨æ ¼ç´¢å¼•"""
        c = self.conn.cursor()
        
        # å–å¾—ç¾æœ‰ç´¢å¼•
        c.execute(f"PRAGMA index_list('{table_name}')")
        indexes = [dict(row) for row in c.fetchall()]
        
        # å–å¾—è¡¨æ ¼çµæ§‹
        c.execute(f"PRAGMA table_info('{table_name}')")
        columns = [dict(row) for row in c.fetchall()]
        
        # å–å¾—è¡¨æ ¼çµ±è¨ˆ
        c.execute(f"SELECT COUNT(*) FROM {table_name}")
        row_count = c.fetchone()[0]
        
        # åˆ†æç´¢å¼•è¦†è“‹
        indexed_columns = set()
        for idx in indexes:
            c.execute(f"PRAGMA index_info('{idx['name']}')")
            for info in c.fetchall():
                indexed_columns.add(info[2])  # column name
        
        # å»ºè­°
        suggestions = []
        
        # æª¢æŸ¥ä¸»éµ
        pk_columns = [col['name'] for col in columns if col['pk']]
        
        # æª¢æŸ¥å¤–éµåˆ—ï¼ˆå¸¸è¦‹å‘½åæ¨¡å¼ï¼‰
        for col in columns:
            name = col['name']
            if name.endswith('_id') and name not in indexed_columns and name not in pk_columns:
                suggestions.append({
                    'type': 'missing_fk_index',
                    'column': name,
                    'reason': 'å¤–éµåˆ—å»ºè­°å»ºç«‹ç´¢å¼•',
                    'sql': f"CREATE INDEX idx_{table_name}_{name} ON {table_name}({name})"
                })
        
        # æª¢æŸ¥å¸¸è¦‹æŸ¥è©¢åˆ—
        common_search_columns = ['status', 'created_at', 'updated_at', 'type', 'category']
        for col_name in common_search_columns:
            if any(c['name'] == col_name for c in columns) and col_name not in indexed_columns:
                suggestions.append({
                    'type': 'common_column',
                    'column': col_name,
                    'reason': 'å¸¸è¦‹æŸ¥è©¢åˆ—å»ºè­°å»ºç«‹ç´¢å¼•',
                    'sql': f"CREATE INDEX idx_{table_name}_{col_name} ON {table_name}({col_name})"
                })
        
        return {
            'table': table_name,
            'row_count': row_count,
            'indexes': indexes,
            'columns': [c['name'] for c in columns],
            'indexed_columns': list(indexed_columns),
            'suggestions': suggestions
        }
    
    def analyze_all_tables(self) -> List[Dict]:
        """åˆ†ææ‰€æœ‰è¡¨æ ¼"""
        c = self.conn.cursor()
        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
        tables = [row[0] for row in c.fetchall()]
        
        results = []
        for table in tables:
            try:
                results.append(self.analyze_table(table))
            except Exception as e:
                logger.error(f"åˆ†æè¡¨æ ¼ {table} å¤±æ•—: {e}")
        
        return results
    
    def get_all_suggestions(self) -> List[Dict]:
        """å–å¾—æ‰€æœ‰ç´¢å¼•å»ºè­°"""
        analyses = self.analyze_all_tables()
        all_suggestions = []
        for analysis in analyses:
            for suggestion in analysis.get('suggestions', []):
                suggestion['table'] = analysis['table']
                all_suggestions.append(suggestion)
        return all_suggestions


# ============================================================
# 4. æ‰¹é‡æ“ä½œå„ªåŒ–
# ============================================================

class BatchExecutor:
    """æ‰¹é‡åŸ·è¡Œå™¨"""
    
    def __init__(self, conn: sqlite3.Connection, batch_size: int = 1000):
        self.conn = conn
        self.batch_size = batch_size
    
    def bulk_insert(self, table: str, columns: List[str], data: List[tuple]) -> int:
        """æ‰¹é‡æ’å…¥
        
        Args:
            table: è¡¨å
            columns: æ¬„ä½åˆ—è¡¨
            data: æ•¸æ“šåˆ—è¡¨
        
        Returns:
            æ’å…¥çš„è¡Œæ•¸
        """
        if not data:
            return 0
        
        placeholders = ','.join(['?' for _ in columns])
        sql = f"INSERT INTO {table} ({','.join(columns)}) VALUES ({placeholders})"
        
        c = self.conn.cursor()
        total = 0
        
        for i in range(0, len(data), self.batch_size):
            batch = data[i:i + self.batch_size]
            c.executemany(sql, batch)
            total += len(batch)
            
            if i % (self.batch_size * 10) == 0:
                self.conn.commit()
        
        self.conn.commit()
        return total
    
    def bulk_update(self, table: str, updates: List[Dict], key_column: str = 'id') -> int:
        """æ‰¹é‡æ›´æ–°
        
        Args:
            table: è¡¨å
            updates: æ›´æ–°åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« key å’Œè¦æ›´æ–°çš„æ¬„ä½
            key_column: ä¸»éµæ¬„ä½
        
        Returns:
            æ›´æ–°çš„è¡Œæ•¸
        """
        if not updates:
            return 0
        
        c = self.conn.cursor()
        total = 0
        
        for update in updates:
            key_value = update.pop(key_column, None)
            if key_value is None:
                continue
            
            set_clause = ','.join([f"{k}=?" for k in update.keys()])
            sql = f"UPDATE {table} SET {set_clause} WHERE {key_column}=?"
            
            c.execute(sql, (*update.values(), key_value))
            total += c.rowcount
            
            if total % self.batch_size == 0:
                self.conn.commit()
        
        self.conn.commit()
        return total
    
    def bulk_delete(self, table: str, ids: List[int], key_column: str = 'id') -> int:
        """æ‰¹é‡åˆªé™¤"""
        if not ids:
            return 0
        
        c = self.conn.cursor()
        total = 0
        
        for i in range(0, len(ids), self.batch_size):
            batch = ids[i:i + self.batch_size]
            placeholders = ','.join(['?' for _ in batch])
            sql = f"DELETE FROM {table} WHERE {key_column} IN ({placeholders})"
            c.execute(sql, batch)
            total += c.rowcount
        
        self.conn.commit()
        return total


# ============================================================
# 5. æ•ˆèƒ½ç›£æ§
# ============================================================

class PerformanceMonitor:
    """æ•ˆèƒ½ç›£æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'requests': 0,
            'errors': 0,
            'total_time': 0,
            'db_time': 0
        }
        self._start_time = time.time()
        self._lock = threading.Lock()
    
    def record_request(self, duration: float, is_error: bool = False):
        """è¨˜éŒ„è«‹æ±‚"""
        with self._lock:
            self.metrics['requests'] += 1
            self.metrics['total_time'] += duration
            if is_error:
                self.metrics['errors'] += 1
    
    def record_db_time(self, duration: float):
        """è¨˜éŒ„è³‡æ–™åº«æ™‚é–“"""
        with self._lock:
            self.metrics['db_time'] += duration
    
    def get_metrics(self) -> Dict:
        """å–å¾—æ•ˆèƒ½æŒ‡æ¨™"""
        uptime = time.time() - self._start_time
        
        with self._lock:
            requests = self.metrics['requests']
            return {
                'uptime_seconds': round(uptime, 2),
                'total_requests': requests,
                'requests_per_second': round(requests / max(uptime, 1), 2),
                'avg_response_time': round(self.metrics['total_time'] / max(requests, 1) * 1000, 2),
                'error_rate': round(self.metrics['errors'] / max(requests, 1) * 100, 2),
                'db_time_pct': round(self.metrics['db_time'] / max(self.metrics['total_time'], 0.001) * 100, 2)
            }
    
    def reset(self):
        """é‡ç½®æŒ‡æ¨™"""
        with self._lock:
            self.metrics = {
                'requests': 0,
                'errors': 0,
                'total_time': 0,
                'db_time': 0
            }
            self._start_time = time.time()


# å…¨åŸŸæ•ˆèƒ½ç›£æ§å™¨
_performance_monitor = PerformanceMonitor()


# ============================================================
# æ…¢æŸ¥è©¢è¨˜éŒ„å™¨
# ============================================================

class SlowQueryLogger:
    """æ…¢æŸ¥è©¢è¨˜éŒ„å™¨"""
    
    def __init__(self, threshold_ms: float = 100.0):
        self.threshold_ms = threshold_ms
        self._logs: List[Dict] = []
        self._stats = {
            'total_queries': 0,
            'slow_queries': 0,
            'avg_time_ms': 0.0,
            'max_time_ms': 0.0
        }
    
    def log(self, query: str, duration_ms: float, params: tuple = None):
        """è¨˜éŒ„æŸ¥è©¢"""
        self._stats['total_queries'] += 1
        
        # æ›´æ–°å¹³å‡æ™‚é–“
        total = self._stats['avg_time_ms'] * (self._stats['total_queries'] - 1)
        self._stats['avg_time_ms'] = (total + duration_ms) / self._stats['total_queries']
        
        # æ›´æ–°æœ€å¤§æ™‚é–“
        if duration_ms > self._stats['max_time_ms']:
            self._stats['max_time_ms'] = duration_ms
        
        # è¨˜éŒ„æ…¢æŸ¥è©¢
        if duration_ms >= self.threshold_ms:
            self._stats['slow_queries'] += 1
            self._logs.append({
                'query': query[:200],
                'duration_ms': round(duration_ms, 2),
                'timestamp': datetime.now().isoformat(),
                'params': str(params)[:100] if params else None
            })
            
            # åªä¿ç•™æœ€è¿‘ 100 æ¢
            if len(self._logs) > 100:
                self._logs = self._logs[-100:]
    
    def get_stats(self) -> Dict:
        """ç²å–çµ±è¨ˆ"""
        return {
            **self._stats,
            'slow_rate': round(
                self._stats['slow_queries'] / max(self._stats['total_queries'], 1) * 100, 2
            )
        }
    
    def get_logs(self, limit: int = 50) -> List[Dict]:
        """ç²å–æ…¢æŸ¥è©¢æ—¥èªŒ"""
        return self._logs[-limit:]
    
    def clear(self):
        """æ¸…é™¤æ—¥èªŒ"""
        self._logs = []
        self._stats = {
            'total_queries': 0,
            'slow_queries': 0,
            'avg_time_ms': 0.0,
            'max_time_ms': 0.0
        }


# å…¨åŸŸæ…¢æŸ¥è©¢è¨˜éŒ„å™¨
slow_query_logger = SlowQueryLogger(threshold_ms=100.0)


def get_performance_dashboard(db_path: str) -> Dict:
    """ç²å–æ€§èƒ½å„€è¡¨æ¿æ•¸æ“š"""
    import os
    import sqlite3
    
    dashboard = {
        'database': {},
        'queries': {},
        'connections': {},
        'recommendations': []
    }
    
    try:
        # è³‡æ–™åº«è³‡è¨Š
        if os.path.exists(db_path):
            stat = os.stat(db_path)
            dashboard['database'] = {
                'path': db_path,
                'size_mb': round(stat.st_size / 1024 / 1024, 2),
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
            }
            
            # è¡¨çµ±è¨ˆ
            conn = sqlite3.connect(db_path)
            cursor = conn.execute(
                "SELECT COUNT(*) FROM sqlite_master WHERE type='table'"
            )
            dashboard['database']['table_count'] = cursor.fetchone()[0]
            
            # ç´¢å¼•çµ±è¨ˆ
            cursor = conn.execute(
                "SELECT COUNT(*) FROM sqlite_master WHERE type='index'"
            )
            dashboard['database']['index_count'] = cursor.fetchone()[0]
            conn.close()
        
        # æŸ¥è©¢çµ±è¨ˆ
        dashboard['queries'] = slow_query_logger.get_stats()
        
        # é€£æ¥æ± è³‡è¨Š
        pool = _pools.get(db_path)
        if pool:
            dashboard['connections'] = {
                'pool_size': pool.max_connections,
                'available': pool._pool.qsize() if hasattr(pool._pool, 'qsize') else 'N/A'
            }
        
        # å»ºè­°
        if dashboard['queries'].get('slow_rate', 0) > 10:
            dashboard['recommendations'].append('æ…¢æŸ¥è©¢æ¯”ä¾‹éé«˜ï¼Œå»ºè­°æª¢æŸ¥ç´¢å¼•')
        
        if dashboard['database'].get('size_mb', 0) > 500:
            dashboard['recommendations'].append('è³‡æ–™åº«æª”æ¡ˆè¼ƒå¤§ï¼Œå»ºè­°åŸ·è¡Œ VACUUM')
        
    except Exception as e:
        dashboard['error'] = str(e)
    
    return dashboard


def get_performance_metrics() -> Dict:
    """å–å¾—æ•ˆèƒ½æŒ‡æ¨™"""
    return {
        'performance': _performance_monitor.get_metrics(),
        'queries': _query_analyzer.get_statistics(),
        'slow_queries': _query_analyzer.get_slow_queries(10)
    }


# ğŸ“š çŸ¥è­˜é»
# -----------
# 1. é€£æ¥æ± è¨­è¨ˆï¼š
#    - Queue å¯¦ç¾ FIFO
#    - æ‡¶å‰µå»ºï¼šéœ€è¦æ™‚æ‰å‰µå»ºé€£æ¥
#    - ä¸Šé™æ§åˆ¶ï¼šé¿å…éå¤šé€£æ¥
#
# 2. SQLite å„ªåŒ– PRAGMAï¼š
#    - WAL æ¨¡å¼ï¼šæé«˜ä¸¦ç™¼æ€§èƒ½
#    - cache_sizeï¼šå¢åŠ å¿«å–
#    - mmap_sizeï¼šè¨˜æ†¶é«”æ˜ å°„
#
# 3. æŸ¥è©¢åˆ†æï¼š
#    - P95 æ™‚é–“ï¼š95% çš„æŸ¥è©¢ä½æ–¼æ­¤æ™‚é–“
#    - æ…¢æŸ¥è©¢è¨˜éŒ„ï¼šå®šä½æ€§èƒ½ç“¶é ¸
#
# 4. ç´¢å¼•å»ºè­°ï¼š
#    - å¤–éµåˆ—æ‡‰å»ºç´¢å¼•
#    - å¸¸è¦‹æŸ¥è©¢åˆ—ï¼ˆstatus, created_atï¼‰
#    - é¿å…éåº¦ç´¢å¼•
#
# 5. æ‰¹é‡æ“ä½œï¼š
#    - executemany() æ¯”å¾ªç’° execute() å¿«
#    - åˆ†æ‰¹æäº¤é¿å…è¨˜æ†¶é«”çˆ†ç‚¸
#    - å®šæœŸ commit å¹³è¡¡æ€§èƒ½èˆ‡å®‰å…¨
